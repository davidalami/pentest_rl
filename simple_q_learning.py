import re
import random
import math
import pickle


# This is our alphabet from which passwords will be constructed.
alphabet = "abcdefghijklmnopqrstuvwxyz0123456789"
max_password_length = 6


def load_passwords_from_file(file_path):
    """Load top passwords from a given file.

    The expected file format is:
    frequency password

    Args:
    - file_path (str): Path to the file containing passwords.

    Returns:
    - list: List of passwords that match our criteria (6 characters long and
    alphanumeric).
    """
    with open(file_path, "r") as f:
        lines = [x.strip() for x in f.readlines() if len(x.split()) == 2]
        passwords = [
            x.split()[1] for x
            in lines
            if len(x.split()[1]) == max_password_length
               and re.match(r"[a-z0-9]{6,}", x.split()[1])
        ]
    return passwords


top_passwords = load_passwords_from_file("stat_russkiwlst_top_1M.txt")


class PasswordAgent:
    """Reinforcement Learning Agent for Password Cracking.

    This agent uses Q-learning to try and guess passwords.
    """

    def __init__(self, alpha=0.1, gamma=0.95, epsilon=0.5,
                 q_table_path="q_table.dump", top_n=5):
        """Initialize the PasswordAgent.

        Args:
        - alpha (float): Learning rate. Determines how much new information
        overrides old information.
        - gamma (float): Discount factor. Determines the importance of future
        rewards.
        - epsilon (float): Exploration rate. The probability of choosing a
        random action over the best one.
        - q_table_path (str): Path to the saved Q-table (if it exists).
        - top_n (int): Number of top actions to consider during exploitation
        """
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.top_n = top_n
        self.q_table_path = q_table_path
        self.q_table = self._load_q_table()

    def _load_q_table(self):
        """Load the Q-table from disk."""
        try:
            with open(self.q_table_path, "rb") as f:
                return pickle.load(f)
        except FileNotFoundError:
            # Return an empty dictionary if the file doesn't exist
            return {}

    def _save_q_table(self):
        """Save the Q-table to disk."""
        with open(self.q_table_path, "wb") as f:
            pickle.dump(self.q_table, f)

    def _choose_action(self, state):
        """
        Determine the next action to be taken by the agent based on the
        Îµ-greedy policy.

        Parameters:
        - state (str): The current state (password guess).

        Returns:
        - str: The selected action, which is a character from the alphabet,
        representing the
               next character in the password guess.
        """

        if random.uniform(0, 1) < self.epsilon:
            return random.choice(alphabet)
        else:
            q_values = {action: self.q_table.get((state, action), 0) for action
                        in alphabet}
            top_n_actions = sorted(q_values, key=q_values.get, reverse=True)[
                            :self.top_n]
            return random.choice(top_n_actions)

    def _get_q(self, state, action):
        """
        Retrieve the Q-value for a specific state-action pair from the Q-table.

        In the Q-learning algorithm, the Q-table maintains the estimated
        rewards (Q-values)
        associated with every possible state-action combination. These
        Q-values represent the
        expected cumulative future rewards an agent can achieve by taking a
        specific action in a
        given state. Over time, through training, the agent refines these
        Q-values to converge
        towards the optimal policy.

        Parameters:
        - state (str): The state for which the Q-value is to be retrieved.
        - action (str): The action for which the Q-value is to be retrieved.

        Returns:
        - float: The Q-value for the specified state-action pair. If the pair
        is not found in
                 the Q-table, it returns a default value of 0.
        """

        return self.q_table.get((state, action), 0)

    def _learn(self, state, action, reward, next_state):
        """
        Update the Q-value for a specific state-action pair based on the
        Q-learning algorithm.

        Parameters:
        - state (str): The current state (password guess).
        - action (str): The action taken by the agent (next character guessed).
        - reward (float): Reward received after taking the action from the
        current state.
        - next_state (str): The resulting state after the action is taken.

        Returns:
        None. The method updates the Q-table in place.

        Formula:
        --------
        Q(state, action) = old Q-value + alpha * (reward + gamma * max_a Q(
        next state, all actions) - old Q-value)

        Where:
        - alpha: Learning rate, determining the extent of updating the Q-value.
        - gamma: Discount factor, indicating the importance of future rewards.
        - max_a Q(next state, all actions): Maximum Q-value for next state
        over all possible actions.
        """

        old_value = self._get_q(state, action)
        next_max = max([self._get_q(next_state, char) for char in alphabet])
        new_value = old_value + self.alpha * (reward + self.gamma * next_max - old_value)
        self.q_table[(state, action)] = new_value

    def _get_reward(self, state):
        """
        Calculate the reward for a given state (password guess).

        This function determines the reward based on the presence of the
        guessed password
        in the predefined list of top passwords (`top_passwords`). If the
        guessed password
        is in the list, the reward is calculated based on its index in the
        list. Specifically,
        the reward is inversely proportional to the logarithm of its index,
        which means passwords
        closer to the start of the list (more common ones) yield higher
        rewards. If the guessed
        password is not in the list, a small negative reward is returned.
        The constant 50 was chosen arbitrarily such that if the last password
        (1000000th)
         was guessed correctly, the reward is still much greater than the
         penalty for a wrong guess.

        Parameters:
        - state (str): The current password guess.

        Returns:
        - float: The reward value. Positive if the guessed password is in the
        `top_passwords`
                 list, and negative otherwise.

        Example:
        --------
        If `state` is a highly common password and is the 10th in the
        `top_passwords` list,
        the reward will be 50/math.log(10+1) = 20.9. However, if `state` is
        not in the `top_passwords`
        list, the reward will be -0.5.
        """

        if state in top_passwords:
            # Added +1 to avoid log(0)
            return 50 / math.log(top_passwords.index(state) + 1)
        return -0.5

    def train(self, episodes=1000):
        """
        Train the reinforcement learning agent to optimize its password
        guessing strategy.
        """

        for _ in range(episodes):
            state = ""
            # distribute credit for success among the many decisions that
            # have been involved in producing it
            episode_history = []

            for _ in range(max_password_length):
                action = self._choose_action(state)
                episode_history.append((state, action))
                next_state = state + action
                reward = self._get_reward(next_state)

                self._learn(state, action, reward,
                            next_state)  # Regular learning step

                discount_factor = 0.8
                for prev_state, prev_action in reversed(episode_history[:-1]):
                    reward *= discount_factor
                    self.q_table[(prev_state, prev_action)] = reward
                    q_delta = reward - self.q_table.get(
                        (prev_state, prev_action), 0)
                    self.q_table[
                        (prev_state, prev_action)] += self.alpha * q_delta

                state = next_state
        self._save_q_table()

    def test(self, start_state="", num_tests=50):
        """Test the trained agent.

        Args:
        - start_state (str): The starting state (or part of the password)
        from which the agent should start guessing.
        - num_tests (int): Number of passwords to guess during the test.
        """
        for _ in range(num_tests):
            state = start_state
            while len(state) < max_password_length:
                q_values = {action: self._get_q(state, action) for action in
                            alphabet}
                top_n_actions = sorted(q_values, key=q_values.get,
                                       reverse=True)[:self.top_n]
                action = random.choice(top_n_actions)
                state += action
            print(state)


if __name__ == '__main__':
    # Create an instance of the PasswordAgent and train it.
    agent = PasswordAgent()
    agent.train()

    # Test the agent to see how well it guesses passwords.
    agent.test(start_state="se")
    # generates and prints the most probable passwords, e.g.
    # serg01, serg03, serg92, sergey ...
